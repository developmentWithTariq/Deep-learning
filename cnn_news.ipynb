{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "efaj-H7SOr2d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils  import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 65
    },
    "colab_type": "code",
    "id": "isF6aukyOr24",
    "outputId": "4a91b324-2d91-49b9-f3d8-a5b503667388"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "# print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow import keras\n",
    "layers = keras.layers\n",
    "models = keras.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "047ZFo27Or35"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "prLDKkKEOr3Z"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/content/bbc-text .csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "colab_type": "code",
    "id": "wi15b9yIOr3j",
    "outputId": "37ba5a5c-280b-4cde-bda1-818906493fd7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1365</th>\n",
       "      <td>business</td>\n",
       "      <td>j&amp;j agrees $25bn guidant deal pharmaceutical g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1366</th>\n",
       "      <td>tech</td>\n",
       "      <td>broadband in the uk growing fast high-speed ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1367</th>\n",
       "      <td>politics</td>\n",
       "      <td>howard denies split over id cards michael howa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1368</th>\n",
       "      <td>politics</td>\n",
       "      <td>no uk apology  for colonial past the days of b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>sport</td>\n",
       "      <td>anelka  eyes man city departure  striker nicol...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1370 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           category                                               text\n",
       "0              tech  tv future in the hands of viewers with home th...\n",
       "1          business  worldcom boss  left books alone  former worldc...\n",
       "2             sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3             sport  yeading face newcastle in fa cup premiership s...\n",
       "4     entertainment  ocean s twelve raids box office ocean s twelve...\n",
       "...             ...                                                ...\n",
       "1365       business  j&j agrees $25bn guidant deal pharmaceutical g...\n",
       "1366           tech  broadband in the uk growing fast high-speed ne...\n",
       "1367       politics  howard denies split over id cards michael howa...\n",
       "1368       politics  no uk apology  for colonial past the days of b...\n",
       "1369          sport  anelka  eyes man city departure  striker nicol...\n",
       "\n",
       "[1370 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Yf2-3ML_Or3t",
    "outputId": "b3aee04c-6380-41eb-99fe-f4220659681c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1370, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "avpGRqEoOr3z",
    "outputId": "a7017b16-e5e3-4e24-e99c-3ca49850b7a8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>politics</td>\n",
       "      <td>howard hits back at mongrel jibe michael howar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>politics</td>\n",
       "      <td>blair prepares to name poll date tony blair is...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve...\n",
       "5       politics  howard hits back at mongrel jibe michael howar...\n",
       "6       politics  blair prepares to name poll date tony blair is..."
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "r5thGsZ-Or4A",
    "outputId": "b14e309c-7f30-4845-8142-97bc57c37a41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1096\n",
      "Test size: 274\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(data) *.8)\n",
    "print (\"Train size: %d\" % train_size)\n",
    "print (\"Test size: %d\" % (len(data) - train_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 526
    },
    "colab_type": "code",
    "id": "myfiLu7_Or4G",
    "outputId": "e719de15-7641-4b19-a35c-aa8eb64841ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(           category                                               text\n",
       " 0              tech  tv future in the hands of viewers with home th...\n",
       " 1          business  worldcom boss  left books alone  former worldc...\n",
       " 2             sport  tigers wary of farrell  gamble  leicester say ...\n",
       " 3             sport  yeading face newcastle in fa cup premiership s...\n",
       " 4     entertainment  ocean s twelve raids box office ocean s twelve...\n",
       " ...             ...                                                ...\n",
       " 1091          sport  laporte tinkers with team france coach bernard...\n",
       " 1092  entertainment  rocker doherty in on-stage fight rock singer p...\n",
       " 1093  entertainment  arnold congratulated on oscar win oscar-winner...\n",
       " 1094       business  gold falls on imf sale concerns the price of g...\n",
       " 1095          sport  african double in edinburgh world 5000m champi...\n",
       " \n",
       " [1096 rows x 2 columns],\n",
       "            category                                               text\n",
       " 1096           tech  internet boom for gift shopping cyberspace is ...\n",
       " 1097           tech  ask jeeves joins web log market ask jeeves has...\n",
       " 1098  entertainment  bbc denies blackadder tv comeback the bbc has ...\n",
       " 1099          sport  everton s weir cools euro hopes everton defend...\n",
       " 1100       business  bbc poll indicates economic gloom citizens in ...\n",
       " ...             ...                                                ...\n",
       " 1365       business  j&j agrees $25bn guidant deal pharmaceutical g...\n",
       " 1366           tech  broadband in the uk growing fast high-speed ne...\n",
       " 1367       politics  howard denies split over id cards michael howa...\n",
       " 1368       politics  no uk apology  for colonial past the days of b...\n",
       " 1369          sport  anelka  eyes man city departure  striker nicol...\n",
       " \n",
       " [274 rows x 2 columns])"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_test_split(data, train_size):\n",
    "    train = data[:train_size]\n",
    "    test = data[train_size:]\n",
    "    return train, test\n",
    "train_test_split(data,train_size=train_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "COqv-gyuOr4M"
   },
   "outputs": [],
   "source": [
    "train_cat, test_cat = train_test_split(data['category'], train_size)\n",
    "train_text, test_text = train_test_split(data['text'], train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "sJNldC1WOr4R",
    "outputId": "69d5e0b6-4f78-42a8-d9ed-7bfe3977492e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       tv future in the hands of viewers with home th...\n",
       "1       worldcom boss  left books alone  former worldc...\n",
       "2       tigers wary of farrell  gamble  leicester say ...\n",
       "3       yeading face newcastle in fa cup premiership s...\n",
       "4       ocean s twelve raids box office ocean s twelve...\n",
       "                              ...                        \n",
       "1091    laporte tinkers with team france coach bernard...\n",
       "1092    rocker doherty in on-stage fight rock singer p...\n",
       "1093    arnold congratulated on oscar win oscar-winner...\n",
       "1094    gold falls on imf sale concerns the price of g...\n",
       "1095    african double in edinburgh world 5000m champi...\n",
       "Name: text, Length: 1096, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "naXts9isOr4X",
    "outputId": "ff1ad755-634b-4234-de2e-78add5fec35b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       tv future in the hands of viewers with home th...\n",
       "1       worldcom boss  left books alone  former worldc...\n",
       "2       tigers wary of farrell  gamble  leicester say ...\n",
       "3       yeading face newcastle in fa cup premiership s...\n",
       "4       ocean s twelve raids box office ocean s twelve...\n",
       "                              ...                        \n",
       "1091    laporte tinkers with team france coach bernard...\n",
       "1092    rocker doherty in on-stage fight rock singer p...\n",
       "1093    arnold congratulated on oscar win oscar-winner...\n",
       "1094    gold falls on imf sale concerns the price of g...\n",
       "1095    african double in edinburgh world 5000m champi...\n",
       "Name: text, Length: 1096, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_words = 1000\n",
    "tokenize = keras.preprocessing.text.Tokenizer(num_words=max_words, \\\n",
    "                                            char_level=False)\n",
    "train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "cxGx21O0Or4c",
    "outputId": "a962edca-7560-4d74-ed46-9a0ffac30ecc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
       "       0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
       "       1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
       "       1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
       "       0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
       "       0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
       "       0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize.fit_on_texts(train_text) # fit tokenizer to our training text data\n",
    "x_train = tokenize.texts_to_matrix(train_text)\n",
    "x_test = tokenize.texts_to_matrix(test_text)\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "alOLDpGaOr4g"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "TPaOPHvxOr4k",
    "outputId": "4510b4ca-ab5b-42b3-a459-82db6ea2792a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_cat)\n",
    "y_train = encoder.transform(train_cat)\n",
    "y_test = encoder.transform(test_cat)\n",
    "y_test.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "colab_type": "code",
    "id": "GAQyKk4FOr4n",
    "outputId": "e977ab60-a7d7-48a0-9136-16e73a56a598"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converts the labels to a one-hot representation\n",
    "num_classes = np.max(y_train) + 1\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "hDTHfoW3j580",
    "outputId": "23986249-3172-405e-95ab-4934eee570f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "CYwXaTbkOr4q",
    "outputId": "b0e7e558-9f59-43eb-f998-b5d50be2f8df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (1096, 1000)\n",
      "x_test shape: (274, 1000)\n",
      "y_train shape: (1096, 5)\n",
      "y_test shape: (274, 5)\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HThW2ivsOr4u"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 4\n",
    "drop_ratio = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aOb6j39QOr4y"
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, input_shape=(max_words,)))\n",
    "model.add(layers.Dense(212 , activation = 'relu'))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Dense(num_classes))\n",
    "model.add(layers.Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "id": "ab2hWpxfOr41",
    "outputId": "0896fb9a-a148-4201-9af6-75cccda0c5aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 986 samples, validate on 110 samples\n",
      "Epoch 1/4\n",
      "986/986 [==============================] - 0s 199us/sample - loss: 0.4539 - acc: 0.8499 - val_loss: 0.2217 - val_acc: 0.9636\n",
      "Epoch 2/4\n",
      "986/986 [==============================] - 0s 90us/sample - loss: 0.0221 - acc: 0.9980 - val_loss: 0.1971 - val_acc: 0.9636\n",
      "Epoch 3/4\n",
      "986/986 [==============================] - 0s 85us/sample - loss: 0.0035 - acc: 1.0000 - val_loss: 0.2090 - val_acc: 0.9545\n",
      "Epoch 4/4\n",
      "986/986 [==============================] - 0s 78us/sample - loss: 0.0018 - acc: 1.0000 - val_loss: 0.2086 - val_acc: 0.9636\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "7r1khDJjOr46",
    "outputId": "fcab94bc-82fd-4e8c-ed3d-c5fd6658cece"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274/274 [==============================] - 0s 101us/sample - loss: 0.0807 - acc: 0.9745\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test,\\\n",
    "                       batch_size=batch_size, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Fh_rHpL2Or4_",
    "outputId": "cf86474e-f8b8-42ee-afea-eb9a45808161"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.08066684529729133\n",
      "Test accuracy: 0.97445256\n"
     ]
    }
   ],
   "source": [
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "vWo_1_pVOr5E",
    "outputId": "a2d641c4-7d46-4230-d6d3-5e02752a2236"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.predict(x_train[0])\n",
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ru_w2SCCOr5I"
   },
   "source": [
    "# Hyperparameter tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gYoxWHNHOr5J"
   },
   "source": [
    "This is a good time to go back and tweak some parameters such as epoch, batch size, dropout ratio, network structure, activation function, and others, to see if you can improve the accuracy.\n",
    "\n",
    "> Indented block\n",
    "\n",
    "\n",
    "\n",
    "In this particular case, to make it more challenging, I recommend reducing the max words of the call to keras.preprocessing.text.Tokenizer. This will reduce the number of words for each input sample, thus making it more challenging to accurately predict the category. (Notice that not all hyperparameters are necessarily inside the model. This is one such example.)\n",
    "\n",
    "The default was up to 1000 words per article. See what happens when you reduce that number to 200 words, or 50 words, or even fewer. As the evaluation accuracy drops, the effects of your hyperparameter tuning will be more pronounced, with successful adjustments making meaningful improvements to the model performance.\n",
    "\n",
    "To make this process easier to manage, I've encapulated the model definition and training and evaluation calls into one function call. You can add additional parameters as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KWut3Kw6Or5J"
   },
   "outputs": [],
   "source": [
    "#by using hyperprameter tunning\n",
    "\n",
    "def run_experiment(batch_size, epochs, drop_ratio):\n",
    "  print('batch size: {}, epochs: {}, drop_ratio: {}'.format(\n",
    "      batch_size, epochs, drop_ratio))\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(512, input_shape=(max_words,)))\n",
    "  model.add(layers.Activation('relu'))\n",
    "  model.add(layers.Dropout(drop_ratio))\n",
    "  model.add(layers.Dense(num_classes))\n",
    "  model.add(layers.Activation('softmax'))\n",
    "\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "  history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=0,\n",
    "                    validation_split=0.1)\n",
    "  score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=0)\n",
    "  print('\\tTest loss:', score[0])\n",
    "  print('\\tTest accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "colab_type": "code",
    "id": "8Y4zzpZgOr5N",
    "outputId": "c22c3798-5eda-4534-ea02-a7714486454f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 16, epochs: 4, drop_ratio: 0.8\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "\tTest loss: 0.12007814732781291\n",
      "\tTest accuracy: 0.9708029\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "epochs = 4\n",
    "drop_ratio = 0.8\n",
    "history=run_experiment(batch_size, epochs, drop_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 745
    },
    "colab_type": "code",
    "id": "MtcEgzZBOr5Q",
    "outputId": "dd4c8401-2b83-468d-9ca1-8a705943e02c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "internet boom for gift shopping cyberspace is beco ...\n",
      "Actual label:tech\n",
      "Predicted label: tech\n",
      "\n",
      "ask jeeves joins web log market ask jeeves has bou ...\n",
      "Actual label:tech\n",
      "Predicted label: tech\n",
      "\n",
      "bbc denies blackadder tv comeback the bbc has said ...\n",
      "Actual label:entertainment\n",
      "Predicted label: entertainment\n",
      "\n",
      "everton s weir cools euro hopes everton defender d ...\n",
      "Actual label:sport\n",
      "Predicted label: sport\n",
      "\n",
      "bbc poll indicates economic gloom citizens in a ma ...\n",
      "Actual label:business\n",
      "Predicted label: business\n",
      "\n",
      "talks aim to avert pension strike talks aimed at a ...\n",
      "Actual label:politics\n",
      "Predicted label: politics\n",
      "\n",
      "mixed signals from french economy the french econo ...\n",
      "Actual label:business\n",
      "Predicted label: business\n",
      "\n",
      "poll idols  face first hurdles vote for me - itv1  ...\n",
      "Actual label:politics\n",
      "Predicted label: politics\n",
      "\n",
      "unilever shake up as profit slips anglo-dutch cons ...\n",
      "Actual label:business\n",
      "Predicted label: business\n",
      "\n",
      "gazprom  in $36m back-tax claim  the nuclear unit  ...\n",
      "Actual label:business\n",
      "Predicted label: business\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_labels = encoder.classes_\n",
    "\n",
    "for i in range(10):\n",
    "    prediction = model.predict(np.array([x_test[i]]))\n",
    "    predicted_label = text_labels[np.argmax(prediction)]\n",
    "    print(test_text.iloc[i][:50], \"...\")\n",
    "    print('Actual label:' + test_cat.iloc[i])\n",
    "    print(\"Predicted label: \" + predicted_label + \"\\n\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "j1ctgJrZOr5T",
    "outputId": "975048a4-bbcd-4183-f759-d1125a117981"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ultimate game  award for doom 3 sci-fi shooter doom 3 has blasted away the competition at a major games ceremony  the golden joystick awards.  it was the only title to win twice  winning ultimate game of the year and best pc game at the awards  presented by little britain star matt lucas. the much-anticipated sci-fi horror doom 3 shot straight to the top of the uk games charts on its release in august. other winners included grand theft auto: san andreas which took the most wanted for christmas prize. only released last week  it was closely followed by halo 2 and half-life 2  which are expected to be big hits when they are unleashed later this month.  but they missed out on the prize for the most wanted game of 2005  which went to the nintendo title  the legend of zelda. the original doom  released in 1994  heralded a new era in computer games and introduced 3d graphics. it helped to establish the concept of the first-person shooter. doom 3 was developed over four years and is thought to have cost around $15m (£8.3m). the top honour for the best online game of the year went to battlefield vietnam. the chronicles of riddick: escape from butcher bay was handed the unsung hero game of 2004. its release was somewhat eclipsed by doom 3  which was released on the same week. it was  however  very well received by gamers and was praised for its storyline which differed from the film released around the same time. electronic arts was named top publisher of the year  taking the crown from nintendo which won in 2003. the annual awards are voted for by more than 200 000 readers of computer and video games magazines. games awards like this have grown in importance. over the last six years  the uk market for games grew by 100% and was worth a record £1 152m in 2003  according to a recent report by analysts screen digest.'"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text.iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "buSizTKAOr5W",
    "outputId": "1dbdf549-8a3c-4d4d-9af6-2ea9213a62e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.6078136e-04, 9.3300268e-04, 2.3036113e-04, 9.9751377e-01,\n",
       "        3.6206053e-04]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict(np.array([x_test[3]]))\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lq11kTJhOr5a"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "MMhmgUzeOr5e",
    "outputId": "bdf4e024-5deb-46cd-b00f-d3c0c561ba3e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sport'"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_label = text_labels[np.argmax(prediction)]\n",
    "predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "8YWtU4_sOr5i",
    "outputId": "078c2b7f-0992-4f15-cfca-6eda1c2a52c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "everton s weir cools euro hopes everton defender d ...\n",
      "Actual label:sport\n",
      "Predicted label: sport\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(test_text.iloc[3][:50], \"...\")\n",
    "print('Actual label:' + test_cat.iloc[3])\n",
    "print(\"Predicted label: \" + predicted_label + \"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xqpFxGcQqrAz"
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cnn_news.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
