{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tariq15994/Deep-learning/blob/master/cnn_news.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "efaj-H7SOr2d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils  import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "isF6aukyOr24"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# print(os.listdir(\"../input\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output.\n",
        "import itertools\n",
        "import os\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from tensorflow import keras\n",
        "layers = keras.layers\n",
        "models = keras.models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "047ZFo27Or35"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "prLDKkKEOr3Z"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"/content/bbc-text.csv \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "wi15b9yIOr3j",
        "outputId": "8d2f8548-5f22-4a50-aeb1-ec49e0945fe1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tech</td>\n",
              "      <td>tv future in the hands of viewers with home th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>business</td>\n",
              "      <td>worldcom boss  left books alone  former worldc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sport</td>\n",
              "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sport</td>\n",
              "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>entertainment</td>\n",
              "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2220</th>\n",
              "      <td>business</td>\n",
              "      <td>cars pull down us retail figures us retail sal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2221</th>\n",
              "      <td>politics</td>\n",
              "      <td>kilroy unveils immigration policy ex-chatshow ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2222</th>\n",
              "      <td>entertainment</td>\n",
              "      <td>rem announce new glasgow concert us band rem h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2223</th>\n",
              "      <td>politics</td>\n",
              "      <td>how political squabbles snowball it s become c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2224</th>\n",
              "      <td>sport</td>\n",
              "      <td>souness delight at euro progress boss graeme s...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2225 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           category                                               text\n",
              "0              tech  tv future in the hands of viewers with home th...\n",
              "1          business  worldcom boss  left books alone  former worldc...\n",
              "2             sport  tigers wary of farrell  gamble  leicester say ...\n",
              "3             sport  yeading face newcastle in fa cup premiership s...\n",
              "4     entertainment  ocean s twelve raids box office ocean s twelve...\n",
              "...             ...                                                ...\n",
              "2220       business  cars pull down us retail figures us retail sal...\n",
              "2221       politics  kilroy unveils immigration policy ex-chatshow ...\n",
              "2222  entertainment  rem announce new glasgow concert us band rem h...\n",
              "2223       politics  how political squabbles snowball it s become c...\n",
              "2224          sport  souness delight at euro progress boss graeme s...\n",
              "\n",
              "[2225 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yf2-3ML_Or3t",
        "outputId": "97e2e312-79b6-430b-8f3f-eccbac69aecc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2225, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "avpGRqEoOr3z",
        "outputId": "dd9685c7-f2b6-465b-ed5c-205cf5918c0a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tech</td>\n",
              "      <td>tv future in the hands of viewers with home th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>business</td>\n",
              "      <td>worldcom boss  left books alone  former worldc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sport</td>\n",
              "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sport</td>\n",
              "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>entertainment</td>\n",
              "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>politics</td>\n",
              "      <td>howard hits back at mongrel jibe michael howar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>politics</td>\n",
              "      <td>blair prepares to name poll date tony blair is...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        category                                               text\n",
              "0           tech  tv future in the hands of viewers with home th...\n",
              "1       business  worldcom boss  left books alone  former worldc...\n",
              "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
              "3          sport  yeading face newcastle in fa cup premiership s...\n",
              "4  entertainment  ocean s twelve raids box office ocean s twelve...\n",
              "5       politics  howard hits back at mongrel jibe michael howar...\n",
              "6       politics  blair prepares to name poll date tony blair is..."
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "data.head(7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5thGsZ-Or4A",
        "outputId": "7b56a6d9-2ae6-4271-83cc-60dc9f91470f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 1780\n",
            "Test size: 445\n"
          ]
        }
      ],
      "source": [
        "train_size = int(len(data) *.8)\n",
        "print (\"Train size: %d\" % train_size)\n",
        "print (\"Test size: %d\" % (len(data) - train_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myfiLu7_Or4G",
        "outputId": "a2a1f003-3045-4a2e-d53b-d1fc554b1f94"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(           category                                               text\n",
              " 0              tech  tv future in the hands of viewers with home th...\n",
              " 1          business  worldcom boss  left books alone  former worldc...\n",
              " 2             sport  tigers wary of farrell  gamble  leicester say ...\n",
              " 3             sport  yeading face newcastle in fa cup premiership s...\n",
              " 4     entertainment  ocean s twelve raids box office ocean s twelve...\n",
              " ...             ...                                                ...\n",
              " 1775           tech  china  blocks google news site  china has been...\n",
              " 1776       politics  labour accused of  eu propaganda  a  taxpayer ...\n",
              " 1777          sport  tevez - an argentine in brazil some 65 years a...\n",
              " 1778       business  water firm suez in argentina row a conflict be...\n",
              " 1779  entertainment  tv presenter deeley drops cd:uk cat deeley has...\n",
              " \n",
              " [1780 rows x 2 columns],\n",
              "            category                                               text\n",
              " 1780  entertainment  hobbit picture  four years away  lord of the r...\n",
              " 1781           tech  game firm holds  cast  auditions video game fi...\n",
              " 1782       politics  clarke plans migrant point scheme anyone plann...\n",
              " 1783          sport  radcliffe will compete in london paula radclif...\n",
              " 1784          sport  serena becomes world number two serena william...\n",
              " ...             ...                                                ...\n",
              " 2220       business  cars pull down us retail figures us retail sal...\n",
              " 2221       politics  kilroy unveils immigration policy ex-chatshow ...\n",
              " 2222  entertainment  rem announce new glasgow concert us band rem h...\n",
              " 2223       politics  how political squabbles snowball it s become c...\n",
              " 2224          sport  souness delight at euro progress boss graeme s...\n",
              " \n",
              " [445 rows x 2 columns])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "def train_test_split(data, train_size):\n",
        "    train = data[:train_size]\n",
        "    test = data[train_size:]\n",
        "    return train, test\n",
        "train_test_split(data,train_size=train_size) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "COqv-gyuOr4M"
      },
      "outputs": [],
      "source": [
        "train_cat, test_cat = train_test_split(data['category'], train_size)\n",
        "train_text, test_text = train_test_split(data['text'], train_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJNldC1WOr4R",
        "outputId": "975eb458-78b0-47db-a932-00d92ae33f49"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       tv future in the hands of viewers with home th...\n",
              "1       worldcom boss  left books alone  former worldc...\n",
              "2       tigers wary of farrell  gamble  leicester say ...\n",
              "3       yeading face newcastle in fa cup premiership s...\n",
              "4       ocean s twelve raids box office ocean s twelve...\n",
              "                              ...                        \n",
              "1775    china  blocks google news site  china has been...\n",
              "1776    labour accused of  eu propaganda  a  taxpayer ...\n",
              "1777    tevez - an argentine in brazil some 65 years a...\n",
              "1778    water firm suez in argentina row a conflict be...\n",
              "1779    tv presenter deeley drops cd:uk cat deeley has...\n",
              "Name: text, Length: 1780, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "train_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naXts9isOr4X",
        "outputId": "38f7ef2b-b525-4dd1-b053-544b59019d78"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       tv future in the hands of viewers with home th...\n",
              "1       worldcom boss  left books alone  former worldc...\n",
              "2       tigers wary of farrell  gamble  leicester say ...\n",
              "3       yeading face newcastle in fa cup premiership s...\n",
              "4       ocean s twelve raids box office ocean s twelve...\n",
              "                              ...                        \n",
              "1775    china  blocks google news site  china has been...\n",
              "1776    labour accused of  eu propaganda  a  taxpayer ...\n",
              "1777    tevez - an argentine in brazil some 65 years a...\n",
              "1778    water firm suez in argentina row a conflict be...\n",
              "1779    tv presenter deeley drops cd:uk cat deeley has...\n",
              "Name: text, Length: 1780, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "max_words = 1000\n",
        "tokenize = keras.preprocessing.text.Tokenizer(num_words=max_words, \\\n",
        "                                            char_level=False)\n",
        "train_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxGx21O0Or4c",
        "outputId": "f18069a5-017a-49b9-80e5-fbfcf203f956"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1.,\n",
              "       0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,\n",
              "       0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
              "       1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1.,\n",
              "       1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
              "       1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
              "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0.,\n",
              "       1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
              "       0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
              "       0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
              "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
              "       1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
              "       0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
              "       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
              "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "tokenize.fit_on_texts(train_text) # fit tokenizer to our training text data\n",
        "x_train = tokenize.texts_to_matrix(train_text)\n",
        "x_test = tokenize.texts_to_matrix(test_text)\n",
        "x_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "alOLDpGaOr4g"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPaOPHvxOr4k",
        "outputId": "f3e95ff4-c003-41c2-ecb1-286fbbb25a6f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "encoder = LabelEncoder()\n",
        "encoder.fit(train_cat)\n",
        "y_train = encoder.transform(train_cat)\n",
        "y_test = encoder.transform(test_cat)\n",
        "y_test.ndim "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAQyKk4FOr4n",
        "outputId": "fecd2497-8e07-4090-e9e8-492205fe36ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 1., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# Converts the labels to a one-hot representation\n",
        "num_classes = np.max(y_train) + 1\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "y_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDTHfoW3j580",
        "outputId": "242c3236-e43e-46f2-9f9b-f4a3791d8214"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYwXaTbkOr4q",
        "outputId": "7d7fe1cc-cc37-4777-dcee-23773994603c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (1780, 1000)\n",
            "x_test shape: (445, 1000)\n",
            "y_train shape: (1780, 5)\n",
            "y_test shape: (445, 5)\n"
          ]
        }
      ],
      "source": [
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "HThW2ivsOr4u"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "epochs = 4\n",
        "drop_ratio = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "aOb6j39QOr4y"
      },
      "outputs": [],
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(512, input_shape=(max_words,)))\n",
        "model.add(layers.Dense(212 , activation = 'relu'))\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.Dense(num_classes))\n",
        "model.add(layers.Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab2hWpxfOr41",
        "outputId": "f2c79924-73e3-411c-d19f-dc3cfb87c572"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "51/51 [==============================] - 3s 8ms/step - loss: 0.3408 - accuracy: 0.8920 - val_loss: 0.2130 - val_accuracy: 0.9382\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0634 - accuracy: 0.9794 - val_loss: 0.1642 - val_accuracy: 0.9326\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 0.0062 - accuracy: 0.9994 - val_loss: 0.1172 - val_accuracy: 0.9607\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.1241 - val_accuracy: 0.9607\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_split=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7r1khDJjOr46",
        "outputId": "fcab94bc-82fd-4e8c-ed3d-c5fd6658cece"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "274/274 [==============================] - 0s 101us/sample - loss: 0.0807 - acc: 0.9745\n"
          ]
        }
      ],
      "source": [
        "score = model.evaluate(x_test, y_test,\\\n",
        "                       batch_size=batch_size, verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "Fh_rHpL2Or4_",
        "outputId": "cf86474e-f8b8-42ee-afea-eb9a45808161"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 0.08066684529729133\n",
            "Test accuracy: 0.97445256\n"
          ]
        }
      ],
      "source": [
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vWo_1_pVOr5E",
        "outputId": "a2d641c4-7d46-4230-d6d3-5e02752a2236"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1000,)"
            ]
          },
          "execution_count": 40,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# model.predict(x_train[0])\n",
        "x_train[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ru_w2SCCOr5I"
      },
      "source": [
        "# Hyperparameter tunning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYoxWHNHOr5J"
      },
      "source": [
        "This is a good time to go back and tweak some parameters such as epoch, batch size, dropout ratio, network structure, activation function, and others, to see if you can improve the accuracy.\n",
        "\n",
        "> Indented block\n",
        "\n",
        "\n",
        "\n",
        "In this particular case, to make it more challenging, I recommend reducing the max words of the call to keras.preprocessing.text.Tokenizer. This will reduce the number of words for each input sample, thus making it more challenging to accurately predict the category. (Notice that not all hyperparameters are necessarily inside the model. This is one such example.)\n",
        "\n",
        "The default was up to 1000 words per article. See what happens when you reduce that number to 200 words, or 50 words, or even fewer. As the evaluation accuracy drops, the effects of your hyperparameter tuning will be more pronounced, with successful adjustments making meaningful improvements to the model performance.\n",
        "\n",
        "To make this process easier to manage, I've encapulated the model definition and training and evaluation calls into one function call. You can add additional parameters as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "KWut3Kw6Or5J"
      },
      "outputs": [],
      "source": [
        "#by using hyperprameter tunning\n",
        "\n",
        "def run_experiment(batch_size, epochs, drop_ratio):\n",
        "  print('batch size: {}, epochs: {}, drop_ratio: {}'.format(\n",
        "      batch_size, epochs, drop_ratio))\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(512, input_shape=(max_words,)))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.Dropout(drop_ratio))\n",
        "  model.add(layers.Dense(num_classes))\n",
        "  model.add(layers.Activation('softmax'))\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])\n",
        "  history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=0,\n",
        "                    validation_split=0.1)\n",
        "  score = model.evaluate(x_test, y_test,\n",
        "                       batch_size=batch_size, verbose=0)\n",
        "  print('\\tTest loss:', score[0])\n",
        "  print('\\tTest accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Y4zzpZgOr5N",
        "outputId": "48eb85ec-baf4-4faf-a7a6-482ccf547faa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch size: 16, epochs: 4, drop_ratio: 0.5\n",
            "\tTest loss: 0.1283823847770691\n",
            "\tTest accuracy: 0.9595505595207214\n"
          ]
        }
      ],
      "source": [
        "batch_size = 16\n",
        "epochs = 4\n",
        "drop_ratio = 0.5\n",
        "history=run_experiment(batch_size, epochs, drop_ratio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        },
        "id": "MtcEgzZBOr5Q",
        "outputId": "dd4c8401-2b83-468d-9ca1-8a705943e02c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "internet boom for gift shopping cyberspace is beco ...\n",
            "Actual label:tech\n",
            "Predicted label: tech\n",
            "\n",
            "ask jeeves joins web log market ask jeeves has bou ...\n",
            "Actual label:tech\n",
            "Predicted label: tech\n",
            "\n",
            "bbc denies blackadder tv comeback the bbc has said ...\n",
            "Actual label:entertainment\n",
            "Predicted label: entertainment\n",
            "\n",
            "everton s weir cools euro hopes everton defender d ...\n",
            "Actual label:sport\n",
            "Predicted label: sport\n",
            "\n",
            "bbc poll indicates economic gloom citizens in a ma ...\n",
            "Actual label:business\n",
            "Predicted label: business\n",
            "\n",
            "talks aim to avert pension strike talks aimed at a ...\n",
            "Actual label:politics\n",
            "Predicted label: politics\n",
            "\n",
            "mixed signals from french economy the french econo ...\n",
            "Actual label:business\n",
            "Predicted label: business\n",
            "\n",
            "poll idols  face first hurdles vote for me - itv1  ...\n",
            "Actual label:politics\n",
            "Predicted label: politics\n",
            "\n",
            "unilever shake up as profit slips anglo-dutch cons ...\n",
            "Actual label:business\n",
            "Predicted label: business\n",
            "\n",
            "gazprom  in $36m back-tax claim  the nuclear unit  ...\n",
            "Actual label:business\n",
            "Predicted label: business\n",
            "\n"
          ]
        }
      ],
      "source": [
        "text_labels = encoder.classes_\n",
        "\n",
        "for i in range(10):\n",
        "    prediction = model.predict(np.array([x_test[i]]))\n",
        "    predicted_label = text_labels[np.argmax(prediction)]\n",
        "    print(test_text.iloc[i][:50], \"...\")\n",
        "    print('Actual label:' + test_cat.iloc[i])\n",
        "    print(\"Predicted label: \" + predicted_label + \"\\n\")  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "j1ctgJrZOr5T",
        "outputId": "975048a4-bbcd-4183-f759-d1125a117981"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'ultimate game  award for doom 3 sci-fi shooter doom 3 has blasted away the competition at a major games ceremony  the golden joystick awards.  it was the only title to win twice  winning ultimate game of the year and best pc game at the awards  presented by little britain star matt lucas. the much-anticipated sci-fi horror doom 3 shot straight to the top of the uk games charts on its release in august. other winners included grand theft auto: san andreas which took the most wanted for christmas prize. only released last week  it was closely followed by halo 2 and half-life 2  which are expected to be big hits when they are unleashed later this month.  but they missed out on the prize for the most wanted game of 2005  which went to the nintendo title  the legend of zelda. the original doom  released in 1994  heralded a new era in computer games and introduced 3d graphics. it helped to establish the concept of the first-person shooter. doom 3 was developed over four years and is thought to have cost around $15m (Â£8.3m). the top honour for the best online game of the year went to battlefield vietnam. the chronicles of riddick: escape from butcher bay was handed the unsung hero game of 2004. its release was somewhat eclipsed by doom 3  which was released on the same week. it was  however  very well received by gamers and was praised for its storyline which differed from the film released around the same time. electronic arts was named top publisher of the year  taking the crown from nintendo which won in 2003. the annual awards are voted for by more than 200 000 readers of computer and video games magazines. games awards like this have grown in importance. over the last six years  the uk market for games grew by 100% and was worth a record Â£1 152m in 2003  according to a recent report by analysts screen digest.'"
            ]
          },
          "execution_count": 44,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_text.iloc[5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "buSizTKAOr5W",
        "outputId": "1dbdf549-8a3c-4d4d-9af6-2ea9213a62e9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[9.6078136e-04, 9.3300268e-04, 2.3036113e-04, 9.9751377e-01,\n",
              "        3.6206053e-04]], dtype=float32)"
            ]
          },
          "execution_count": 54,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prediction = model.predict(np.array([x_test[3]]))\n",
        "prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lq11kTJhOr5a"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MMhmgUzeOr5e",
        "outputId": "bdf4e024-5deb-46cd-b00f-d3c0c561ba3e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'sport'"
            ]
          },
          "execution_count": 55,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predicted_label = text_labels[np.argmax(prediction)]\n",
        "predicted_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "8YWtU4_sOr5i",
        "outputId": "078c2b7f-0992-4f15-cfca-6eda1c2a52c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "everton s weir cools euro hopes everton defender d ...\n",
            "Actual label:sport\n",
            "Predicted label: sport\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(test_text.iloc[3][:50], \"...\")\n",
        "print('Actual label:' + test_cat.iloc[3])\n",
        "print(\"Predicted label: \" + predicted_label + \"\\n\") "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "now i am going to reduce the max word from 1000 to 100"
      ],
      "metadata": {
        "id": "cL28IphMya56"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "xqpFxGcQqrAz"
      },
      "outputs": [],
      "source": [
        "new_max_words = 100\n",
        "tokenize = keras.preprocessing.text.Tokenizer(num_words=new_max_words, \\\n",
        "                                            char_level=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize.fit_on_texts(train_text) # fit tokenizer to our training text data\n",
        "x_train = tokenize.texts_to_matrix(train_text)\n",
        "x_test = tokenize.texts_to_matrix(test_text)"
      ],
      "metadata": {
        "id": "xxSYFoNrzjS-"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def run_experiment(batch_size, epochs, drop_ratio):\n",
        "  print('batch size: {}, epochs: {}, drop_ratio: {}'.format(\n",
        "      batch_size, epochs, drop_ratio))\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(512, input_shape=(new_max_words,)))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.Dropout(drop_ratio))\n",
        "  model.add(layers.Dense(num_classes))\n",
        "  model.add(layers.Activation('softmax'))\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])\n",
        "  history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=0,\n",
        "                    validation_split=0.1)\n",
        "  score = model.evaluate(x_test, y_test,\n",
        "                       batch_size=batch_size, verbose=0)\n",
        "  print('\\tTest loss:', score[0])\n",
        "  print('\\tTest accuracy:', score[1])"
      ],
      "metadata": {
        "id": "Fjse3-jJ0SPr"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we are training on same model with 100 max words\n",
        "batch_size = 16\n",
        "epochs = 4\n",
        "drop_ratio = 0.5\n",
        "history=run_experiment(batch_size, epochs, drop_ratio)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XN9Zs4b0z2PE",
        "outputId": "7a963b8f-5abb-4684-a66b-a6f76fac6396"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch size: 16, epochs: 4, drop_ratio: 0.5\n",
            "\tTest loss: 0.548629105091095\n",
            "\tTest accuracy: 0.802247166633606\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now just add to normalization layers and increasing the epoch size to 30\n",
        "\n",
        "def run_experiment(batch_size, epochs, drop_ratio):\n",
        "  print('batch size: {}, epochs: {}, drop_ratio: {}'.format(\n",
        "      batch_size, epochs, drop_ratio))\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(512, input_shape=(new_max_words,)))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.Dropout(drop_ratio))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Dense(num_classes))\n",
        "  model.add(layers.Activation('softmax'))\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])\n",
        "  history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_split=0.1)\n",
        "  score = model.evaluate(x_test, y_test,\n",
        "                       batch_size=batch_size, verbose=0)\n",
        "  print('\\tTest loss:', score[0])\n",
        "  print('\\tTest accuracy:', score[1])"
      ],
      "metadata": {
        "id": "E9DEHbJL0Iga"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "epochs = 30\n",
        "drop_ratio = 0.5\n",
        "history=run_experiment(batch_size, epochs, drop_ratio)\n",
        "# its not impacting more on perforance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYtReYwf09i3",
        "outputId": "5061cf2f-0c69-42f8-aedb-8f90db6fe276"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch size: 16, epochs: 30, drop_ratio: 0.5\n",
            "Epoch 1/30\n",
            "101/101 [==============================] - 2s 8ms/step - loss: 1.1962 - accuracy: 0.5630 - val_loss: 0.9768 - val_accuracy: 0.6742\n",
            "Epoch 2/30\n",
            "101/101 [==============================] - 1s 6ms/step - loss: 0.7507 - accuracy: 0.7228 - val_loss: 0.6256 - val_accuracy: 0.8090\n",
            "Epoch 3/30\n",
            "101/101 [==============================] - 1s 6ms/step - loss: 0.6117 - accuracy: 0.7790 - val_loss: 0.6031 - val_accuracy: 0.7753\n",
            "Epoch 4/30\n",
            "101/101 [==============================] - 1s 6ms/step - loss: 0.5597 - accuracy: 0.7946 - val_loss: 0.5151 - val_accuracy: 0.8146\n",
            "Epoch 5/30\n",
            "101/101 [==============================] - 1s 6ms/step - loss: 0.4950 - accuracy: 0.8346 - val_loss: 0.5323 - val_accuracy: 0.7978\n",
            "Epoch 6/30\n",
            "101/101 [==============================] - 1s 6ms/step - loss: 0.4625 - accuracy: 0.8227 - val_loss: 0.5452 - val_accuracy: 0.8146\n",
            "Epoch 7/30\n",
            "101/101 [==============================] - 1s 6ms/step - loss: 0.4662 - accuracy: 0.8196 - val_loss: 0.5849 - val_accuracy: 0.7978\n",
            "Epoch 8/30\n",
            "101/101 [==============================] - 1s 6ms/step - loss: 0.4050 - accuracy: 0.8577 - val_loss: 0.4734 - val_accuracy: 0.8427\n",
            "Epoch 9/30\n",
            "101/101 [==============================] - 1s 6ms/step - loss: 0.3881 - accuracy: 0.8564 - val_loss: 0.4934 - val_accuracy: 0.8315\n",
            "Epoch 10/30\n",
            "101/101 [==============================] - 1s 6ms/step - loss: 0.3652 - accuracy: 0.8577 - val_loss: 0.5267 - val_accuracy: 0.8258\n",
            "Epoch 11/30\n",
            "101/101 [==============================] - 1s 6ms/step - loss: 0.3424 - accuracy: 0.8876 - val_loss: 0.5023 - val_accuracy: 0.8146\n",
            "Epoch 12/30\n",
            "101/101 [==============================] - 1s 6ms/step - loss: 0.3299 - accuracy: 0.8795 - val_loss: 0.5821 - val_accuracy: 0.8146\n",
            "Epoch 13/30\n",
            "101/101 [==============================] - 1s 6ms/step - loss: 0.2992 - accuracy: 0.8976 - val_loss: 0.5255 - val_accuracy: 0.8258\n",
            "Epoch 14/30\n",
            "101/101 [==============================] - 1s 6ms/step - loss: 0.2971 - accuracy: 0.8958 - val_loss: 0.5742 - val_accuracy: 0.8090\n",
            "Epoch 15/30\n",
            "101/101 [==============================] - 1s 6ms/step - loss: 0.2946 - accuracy: 0.8976 - val_loss: 0.5664 - val_accuracy: 0.7978\n",
            "Epoch 16/30\n",
            "101/101 [==============================] - 1s 6ms/step - loss: 0.2836 - accuracy: 0.8945 - val_loss: 0.6154 - val_accuracy: 0.8090\n",
            "Epoch 17/30\n",
            "101/101 [==============================] - 1s 6ms/step - loss: 0.2592 - accuracy: 0.9114 - val_loss: 0.5408 - val_accuracy: 0.8315\n",
            "Epoch 18/30\n",
            "101/101 [==============================] - 1s 6ms/step - loss: 0.2657 - accuracy: 0.9107 - val_loss: 0.5200 - val_accuracy: 0.8371\n",
            "Epoch 19/30\n",
            "101/101 [==============================] - 1s 6ms/step - loss: 0.2583 - accuracy: 0.9039 - val_loss: 0.6045 - val_accuracy: 0.8090\n",
            "Epoch 20/30\n",
            "101/101 [==============================] - 1s 6ms/step - loss: 0.2613 - accuracy: 0.9064 - val_loss: 0.5541 - val_accuracy: 0.8258\n",
            "Epoch 21/30\n",
            "101/101 [==============================] - 1s 6ms/step - loss: 0.2362 - accuracy: 0.9145 - val_loss: 0.5557 - val_accuracy: 0.8258\n",
            "Epoch 22/30\n",
            "101/101 [==============================] - 1s 6ms/step - loss: 0.2149 - accuracy: 0.9182 - val_loss: 0.5151 - val_accuracy: 0.8258\n",
            "Epoch 23/30\n",
            "101/101 [==============================] - 1s 6ms/step - loss: 0.2407 - accuracy: 0.9170 - val_loss: 0.5874 - val_accuracy: 0.7978\n",
            "Epoch 24/30\n",
            "101/101 [==============================] - 1s 6ms/step - loss: 0.2431 - accuracy: 0.9120 - val_loss: 0.5838 - val_accuracy: 0.8146\n",
            "Epoch 25/30\n",
            "101/101 [==============================] - 1s 6ms/step - loss: 0.2119 - accuracy: 0.9257 - val_loss: 0.5082 - val_accuracy: 0.8371\n",
            "Epoch 26/30\n",
            "101/101 [==============================] - 1s 6ms/step - loss: 0.1901 - accuracy: 0.9351 - val_loss: 0.5388 - val_accuracy: 0.8315\n",
            "Epoch 27/30\n",
            "101/101 [==============================] - 1s 6ms/step - loss: 0.2121 - accuracy: 0.9176 - val_loss: 0.6189 - val_accuracy: 0.8034\n",
            "Epoch 28/30\n",
            "101/101 [==============================] - 1s 6ms/step - loss: 0.2312 - accuracy: 0.9232 - val_loss: 0.6293 - val_accuracy: 0.7978\n",
            "Epoch 29/30\n",
            "101/101 [==============================] - 1s 5ms/step - loss: 0.1938 - accuracy: 0.9263 - val_loss: 0.5825 - val_accuracy: 0.8090\n",
            "Epoch 30/30\n",
            "101/101 [==============================] - 1s 5ms/step - loss: 0.1742 - accuracy: 0.9395 - val_loss: 0.5945 - val_accuracy: 0.8427\n",
            "\tTest loss: 0.6336592435836792\n",
            "\tTest accuracy: 0.8044943809509277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now some test with big model\n",
        "\n",
        "\n",
        "def Conv1d_experiment(batch_size, epochs, drop_ratio):\n",
        "  print('batch size: {}, epochs: {}, drop_ratio: {}'.format(\n",
        "      batch_size, epochs, drop_ratio))\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(100, input_shape=(new_max_words,)))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.Dropout(drop_ratio))\n",
        "  model.add(layers.Dense(50,activation='relu'))\n",
        "  model.add(layers.Dropout(0.3))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Dense(20,activation='relu'))\n",
        "  model.add(layers.Dense(num_classes))\n",
        "  model.add(layers.Activation('softmax'))\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_split=0.1)\n",
        "  score = model.evaluate(x_test, y_test,\n",
        "                       batch_size=batch_size, verbose=0)\n",
        "  print('\\tTest loss:', score[0])\n",
        "  print('\\tTest accuracy:', score[1])"
      ],
      "metadata": {
        "id": "Qqbt53Bb1VjT"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "epochs = 30\n",
        "drop_ratio = 0.5\n",
        "history=Conv1d_experiment(batch_size, epochs, drop_ratio)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09nHRmL33zYX",
        "outputId": "2a445e85-ece7-4000-ac69-55b17a9e0e24"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch size: 64, epochs: 30, drop_ratio: 0.5\n",
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_27 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " batch_normalization_14 (Bat  (None, 100)              400       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_22 (Activation)  (None, 100)               0         \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 100)               0         \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 50)                5050      \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 50)                0         \n",
            "                                                                 \n",
            " batch_normalization_15 (Bat  (None, 50)               200       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 20)                1020      \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 5)                 105       \n",
            "                                                                 \n",
            " activation_23 (Activation)  (None, 5)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16,875\n",
            "Trainable params: 16,575\n",
            "Non-trainable params: 300\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "26/26 [==============================] - 1s 16ms/step - loss: 1.8940 - accuracy: 0.2266 - val_loss: 1.5459 - val_accuracy: 0.2753\n",
            "Epoch 2/30\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 1.5633 - accuracy: 0.3340 - val_loss: 1.5000 - val_accuracy: 0.3427\n",
            "Epoch 3/30\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 1.4768 - accuracy: 0.3826 - val_loss: 1.4262 - val_accuracy: 0.4494\n",
            "Epoch 4/30\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 1.3558 - accuracy: 0.4526 - val_loss: 1.3195 - val_accuracy: 0.5506\n",
            "Epoch 5/30\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 1.2798 - accuracy: 0.4888 - val_loss: 1.2111 - val_accuracy: 0.6124\n",
            "Epoch 6/30\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 1.1720 - accuracy: 0.5587 - val_loss: 1.1030 - val_accuracy: 0.6685\n",
            "Epoch 7/30\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 1.1157 - accuracy: 0.5799 - val_loss: 1.0087 - val_accuracy: 0.7079\n",
            "Epoch 8/30\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 1.0444 - accuracy: 0.6042 - val_loss: 0.9022 - val_accuracy: 0.7528\n",
            "Epoch 9/30\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 1.0184 - accuracy: 0.6317 - val_loss: 0.8425 - val_accuracy: 0.7584\n",
            "Epoch 10/30\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.9634 - accuracy: 0.6417 - val_loss: 0.7886 - val_accuracy: 0.7697\n",
            "Epoch 11/30\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.9175 - accuracy: 0.6592 - val_loss: 0.7336 - val_accuracy: 0.7809\n",
            "Epoch 12/30\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.8476 - accuracy: 0.6935 - val_loss: 0.7124 - val_accuracy: 0.7528\n",
            "Epoch 13/30\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.8371 - accuracy: 0.6929 - val_loss: 0.6777 - val_accuracy: 0.7753\n",
            "Epoch 14/30\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.7895 - accuracy: 0.7235 - val_loss: 0.6610 - val_accuracy: 0.7640\n",
            "Epoch 15/30\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.7595 - accuracy: 0.7147 - val_loss: 0.6552 - val_accuracy: 0.7697\n",
            "Epoch 16/30\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.7499 - accuracy: 0.7247 - val_loss: 0.6290 - val_accuracy: 0.7809\n",
            "Epoch 17/30\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.7463 - accuracy: 0.7266 - val_loss: 0.6078 - val_accuracy: 0.8034\n",
            "Epoch 18/30\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.7065 - accuracy: 0.7491 - val_loss: 0.6107 - val_accuracy: 0.7921\n",
            "Epoch 19/30\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6802 - accuracy: 0.7478 - val_loss: 0.6111 - val_accuracy: 0.7978\n",
            "Epoch 20/30\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.6509 - accuracy: 0.7747 - val_loss: 0.5927 - val_accuracy: 0.8090\n",
            "Epoch 21/30\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6617 - accuracy: 0.7547 - val_loss: 0.5924 - val_accuracy: 0.7978\n",
            "Epoch 22/30\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6532 - accuracy: 0.7684 - val_loss: 0.5739 - val_accuracy: 0.8090\n",
            "Epoch 23/30\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6293 - accuracy: 0.7740 - val_loss: 0.5629 - val_accuracy: 0.8202\n",
            "Epoch 24/30\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6132 - accuracy: 0.7859 - val_loss: 0.5463 - val_accuracy: 0.8090\n",
            "Epoch 25/30\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.5967 - accuracy: 0.7846 - val_loss: 0.5558 - val_accuracy: 0.8034\n",
            "Epoch 26/30\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6116 - accuracy: 0.7703 - val_loss: 0.5466 - val_accuracy: 0.8034\n",
            "Epoch 27/30\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6000 - accuracy: 0.7871 - val_loss: 0.5462 - val_accuracy: 0.8034\n",
            "Epoch 28/30\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5604 - accuracy: 0.7971 - val_loss: 0.5339 - val_accuracy: 0.8146\n",
            "Epoch 29/30\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5631 - accuracy: 0.7878 - val_loss: 0.5211 - val_accuracy: 0.8202\n",
            "Epoch 30/30\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.5632 - accuracy: 0.8040 - val_loss: 0.5018 - val_accuracy: 0.8090\n",
            "\tTest loss: 0.5584076642990112\n",
            "\tTest accuracy: 0.8044943809509277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets create model using functional modeling"
      ],
      "metadata": {
        "id": "Oo_bw4Ag6YYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense , Dropout , BatchNormalization, Input\n",
        "from keras.models import Model\n",
        "# x = Model()\n",
        "x1 = Input((1000))"
      ],
      "metadata": {
        "id": "eF0huw4w4C4V"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "cnn_news.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}